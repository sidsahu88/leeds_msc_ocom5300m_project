{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOkw6kQKz+/ajn6WVaIKYr+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Environment Setup"],"metadata":{"id":"HCBOs6kKz5ke"}},{"cell_type":"code","source":["pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_FT7bz1LSr_","executionInfo":{"status":"ok","timestamp":1702138393759,"user_tz":300,"elapsed":7652,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"outputId":"60decbe3-5162-451e-af18-b396b9d97ab8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets==2.15.0 (from -r requirements.txt (line 1))\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting thop==0.1.1.post2209072238 (from -r requirements.txt (line 2))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets==2.15.0->-r requirements.txt (line 1))\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0->-r requirements.txt (line 1))\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (3.4.1)\n","Collecting multiprocess (from datasets==2.15.0->-r requirements.txt (line 1))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (2.1.0+cu118)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 1)) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0->-r requirements.txt (line 1)) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 1)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 1)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 1)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 1)) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0->-r requirements.txt (line 1)) (2023.3.post1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.15.0->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop==0.1.1.post2209072238->-r requirements.txt (line 2)) (1.3.0)\n","Installing collected packages: pyarrow-hotfix, dill, multiprocess, thop, datasets\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 thop-0.1.1.post2209072238\n"]}]},{"cell_type":"markdown","source":["Importing required libraries"],"metadata":{"id":"O8M_r6hdzyct"}},{"cell_type":"code","source":["from capsule_network import *\n","from dataloader import image_dataloader\n","\n","from PIL import Image\n","\n","import torch\n","from torch.optim import Adam\n","\n","from datasets import load_dataset\n","\n","from scipy.stats import pearsonr, spearmanr, kendalltau\n","from thop import profile"],"metadata":{"id":"U1GSRV09zxjw","executionInfo":{"status":"ok","timestamp":1702138406452,"user_tz":300,"elapsed":12695,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e37e9c9e-a497-4908-be61-4135ccec980d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"KyAPzJ3eiyPV"},"source":["Device (CPU/GPU) available to use."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1702138406453,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"},"user_tz":300},"id":"KV2qBbsRiyPW","outputId":"ffd49f43-84d2-4fa9-a695-18ed3ce5a164"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"markdown","source":["## DataLoaders"],"metadata":{"id":"xWgd5GNc3VtI"}},{"cell_type":"code","source":["class TinyImageNetDataset(Dataset):\n","    def __init__(self, split='train'):\n","        '''\n","        split: Valid values are ['train', 'valid']\n","        '''\n","        self.transform = transforms.Compose([\n","                            transforms.ToTensor(),\n","                            transforms.Resize(64),      # Size of a tiny-imagenet image: (64, 64)\n","                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","\n","        self.dataset = load_dataset('Maysee/tiny-imagenet', split=split)\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, index):\n","        image = self.dataset[index]['image']\n","        image = image.convert(\"RGB\")\n","        transformed_img = self.transform(image)\n","        return transformed_img, self.dataset[index]['label']"],"metadata":{"id":"yMfNPC2C8Zz6","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":180,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def image_dataloader(dataset='mnist', batch_size=100):\n","    if dataset == 'mnist':\n","        transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.1307,), (0.3081,))])\n","\n","        trainset = datasets.MNIST('/data/mnist', train=True, download=True, transform=transform)\n","        testset = datasets.MNIST('/data/mnist', train=False, download=True, transform=transform)\n","\n","    elif dataset == 'cifar10':\n","        transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(32),      # Size of a cifar10 image: (32, 32)\n","                transforms.Normalize((0.485, 0.456, 0.406) , (0.229, 0.224, 0.225))])\n","\n","        trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","        testset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n","\n","    elif dataset == 'tinyimagenet':\n","        trainset = TinyImageNetDataset(split='train')\n","        testset = TinyImageNetDataset(split='valid')\n","\n","    else:\n","        raise ValueError(\"Argument 'dataset' must be one of the values in ['cifar10, 'tinyimagenet'].\")\n","\n","    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"qPHBXuD5cNOk","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":6,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Capsule Network"],"metadata":{"id":"JfwI1y6qqIm9"}},{"cell_type":"markdown","source":["### Convolutional Layer"],"metadata":{"id":"nxHQWdt34YZU"}},{"cell_type":"code","source":["class ConvLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels=256, kernel_size=9, stride=1, *args, **kwargs):\n","        super(ConvLayer, self).__init__(*args, **kwargs)\n","\n","        self.conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n","\n","    def forward(self, x):\n","        return F.relu(self.conv_layer(x))"],"metadata":{"id":"ONqK8qgnmIyR","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":6,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def get_conv_out_dim(img_h, img_w, kernel_size, stride, padding=0):\n","    out_h = (int((img_h + (2*padding) - kernel_size) / stride)) + 1\n","    out_w = (int((img_w + (2*padding) - kernel_size) / stride)) + 1\n","    return out_h, out_w"],"metadata":{"id":"xN-8NclADMiv","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":6,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Primary Capsule Layer"],"metadata":{"id":"4dm8_CTk4gOA"}},{"cell_type":"code","source":["def squash(tensor, dim=-1):\n","    sqr_norm = torch.sum(tensor**2, dim=dim, keepdim=True)\n","    norm = torch.sqrt(sqr_norm)\n","    return (sqr_norm * tensor) / ((1. + sqr_norm) * norm)"],"metadata":{"id":"hWQ-hFyZHzZF","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":5,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class PrimaryCapsLayer(nn.Module):\n","    def __init__(self, in_channels=256, out_channels=32, out_caps_dim=8, kernel_size=9, stride=2, padding='valid', *args, **kwargs):\n","        super(PrimaryCapsLayer, self).__init__(*args, **kwargs)\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.out_caps_dim = out_caps_dim\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.n_prim_caps = None\n","\n","        self.primary_caps = nn.ModuleList(\n","            [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n","             for _ in range(out_caps_dim)])\n","\n","    def forward(self, x):\n","        if self.n_prim_caps is None:\n","            conv_out_dim1, conv_out_dim2 = get_conv_out_dim(x.size(-2),             # Calculate convolutional output dimension for zero padding\n","                                                            x.size(-1),\n","                                                            self.kernel_size,\n","                                                            self.stride)\n","            self.n_prim_caps = conv_out_dim1 * conv_out_dim2 * self.out_channels    # Get number of capsules in the output\n","\n","        batch_size = x.size(0)\n","        u_list = [capsule(x) for capsule in self.primary_caps]  # Output capsule shape: (batch_size, out_channels, conv_out_dim1, conv_out_dim2)\n","        u = torch.stack(u_list, dim=-1)                         # Stacking capsule output to get shape: (batch_size, out_channels, conv_out_dim1, conv_out_dim2, out_caps_dim)\n","        u = u.view(batch_size, self.n_prim_caps, -1)            # Reshape the tensor to (batch_size, prim_caps_out_dim, out_caps_dim)\n","        return squash(u)                                        # Squash returns the tensor shape: (batch_size, prim_caps_out_dim, out_caps_dim)"],"metadata":{"id":"75IPzWD4uQ2H","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":5,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Class Capsule Layer"],"metadata":{"id":"hOOjsdv14lgx"}},{"cell_type":"code","source":["class ClassCapsLayer(nn.Module):\n","    def __init__(self, n_class, n_prim_caps, in_caps_dim=8, out_caps_dim=16, *args, **kwargs):\n","        super(ClassCapsLayer, self).__init__(*args, **kwargs)\n","\n","        self.n_class = n_class\n","        self.n_prim_caps = n_prim_caps\n","        self.in_caps_dim = in_caps_dim\n","        self.out_caps_dim = out_caps_dim\n","\n","        self.weights = nn.Parameter(torch.randn(n_prim_caps, n_class, out_caps_dim, in_caps_dim))\n","\n","    def forward(self, u):\n","        batch_size = u.size(0)\n","        W = torch.stack([self.weights] * batch_size, dim=0)         # W Shape: (batch_size, n_prim_caps, n_class, out_caps_dim, in_caps_dim)\n","        # print(\"W shape: \", W.size())\n","        u = torch.stack([u] * self.n_class, dim=2).unsqueeze(-1)    # u Shape: (batch_size, n_prim_caps, n_class, in_caps_dim, 1)\n","        # print(\"u shape: \", u.size())\n","        u_hat = torch.matmul(W, u)                                  # u_hat Shape: (batch_size, n_prim_caps, n_class, out_caps_dim, 1)\n","        return u_hat"],"metadata":{"id":"GdjPQvwRkMWQ","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":5,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Routing Aggreement"],"metadata":{"id":"bI2rqqQY4o9X"}},{"cell_type":"code","source":["class RoutingByAggreement(nn.Module):\n","    def __init__(self, n_in_caps, n_out_caps,  n_iterations=3, *args, **kwargs):\n","        super(RoutingByAggreement, self).__init__(*args, **kwargs)\n","\n","        self.n_in_caps = n_in_caps\n","        self.n_out_caps = n_out_caps\n","        self.n_iterations = n_iterations\n","\n","    def forward(self, u_hat):\n","        # print(\"u_hat: \", u_hat.size())\n","        batch_size = u_hat.size(0)      # u_hat Shape: (batch_size, n_in_caps, n_out_caps, output_capsule_dim, 1)\n","\n","        bij = Variable(torch.zeros(batch_size, self.n_in_caps, self.n_out_caps, 1, 1, dtype=torch.float, device=device)) # bij Shape: (batch_size, n_in_caps, n_out_caps, 1, 1)\n","        # print(\"bij shape: \", bij.shape)\n","\n","        for r in range(self.n_iterations):\n","            cij = F.softmax(bij, dim=1)                             # cij Shape: (batch_size, n_in_caps, n_out_caps, 1, 1)\n","            # print(\"cij shape: \", cij.shape)\n","            sj = (cij * u_hat).sum(dim=1, keepdim=True)             # sj Shape: (batch_size, 1, n_out_caps, output_capsule_dim, 1)\n","            # print(\"sj shape: \", sj.shape)\n","            vj = squash(sj, dim=-2)                                 # vj Shape: (batch_size, 1, n_out_caps, output_capsule_dim, 1)\n","            vj_in_caps = torch.cat([vj] * self.n_in_caps, dim=1)    # vj Shape: (batch_size, n_in_caps, n_out_caps, output_capsule_dim, 1)\n","            u_hat_T = u_hat.transpose(-2, -1)\n","            # print(\"vj_in_caps shape: \", vj_in_caps.shape)\n","            # print(\"u_hat_T shape: \", u_hat_T.shape)\n","\n","            if r < self.n_iterations - 1:\n","                aij = torch.matmul(u_hat_T, vj_in_caps)             # aij Shape: (batch_size, n_in_caps, n_out_caps, 1, 1)\n","                # print(\"aij shape: \", aij.shape)\n","                bij = bij + aij\n","\n","        # print(\"vj squeezed shape: \", vj.squeeze(1).shape)\n","        return vj.squeeze(1)     # vj Shape Squeezed: (batch_size, n_out_caps, output_capsule_dim, 1)"],"metadata":{"id":"kAj4tpDIzfpy","executionInfo":{"status":"ok","timestamp":1702081051139,"user_tz":300,"elapsed":4,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Decoder Network"],"metadata":{"id":"YWkUFAg14uWn"}},{"cell_type":"code","source":["class CapsDecoder(nn.Module):\n","    def __init__(self, n_class, in_img_c, in_img_h, in_img_w, out_capsule_dim=16, *args, **kwargs):\n","        super(CapsDecoder, self).__init__(*args, **kwargs)\n","\n","        self.n_class = n_class\n","        self.in_img_c = in_img_c\n","        self.in_img_h = in_img_h\n","        self.in_img_w = in_img_w\n","\n","        fcl_input_dim = out_capsule_dim * n_class\n","        fcl_ouput_dim = in_img_c * in_img_h * in_img_w\n","\n","        self.fully_conn_layers = nn.Sequential(\n","            nn.Linear(fcl_input_dim, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, fcl_ouput_dim),\n","            nn.Sigmoid())\n","\n","    def forward(self, v):\n","        # print(\"v shape: \", v.shape)\n","        # print(\"v: \", v[0][0])\n","        caps_out_class = torch.sqrt((v ** 2).sum(dim=2))   # v shape: (batch_size, n_class, out_capsule_dim, 1)\n","        caps_out_class = caps_out_class.squeeze()           # caps_out_class shape: (batch_size, n_class)\n","        # print(\"caps_out_class shape:\", caps_out_class.shape)\n","        # print(\"caps_out_class: \", caps_out_class[0])\n","        caps_out_class = F.softmax(caps_out_class, dim=1)      # caps_out_class Shape: (batch_size, n_class)\n","        # print(\"caps_out_class shape after softmax:\", caps_out_class.shape)\n","        # print(\"caps_out_class: \", caps_out_class[0])\n","        max_class_indices = caps_out_class.argmax(dim=1)\n","        # print(\"max_class_indices: \",max_class_indices)\n","        masked_matrix = Variable(torch.eye(self.n_class, device=device))\n","        masked_matrix = masked_matrix.index_select(dim=0, index=max_class_indices.squeeze())       # masked_matrix Shape: (batch_size, n_class)\n","\n","        v = v * masked_matrix[:, :, None, None]           # v Shape: (batch_size, n_class, out_capsule_dim, 1)\n","        flattened_v = v.view(v.size(0), -1)             # flattened_v Shape: (batch_size, n_class * out_capsule_dim * 1)\n","\n","        reconstructed_img = self.fully_conn_layers(flattened_v)    # reconstructed_img Shape: (batch_size, fcl_ouput_dim)\n","        reconstructed_img = reconstructed_img.view(-1, self.in_img_c, self.in_img_h, self.in_img_w)   # reconstructed_img Shape: (batch_size, in_img_ch, in_img_h, in_img_w)\n","\n","        return reconstructed_img"],"metadata":{"id":"FNjqBK-6EQxB","executionInfo":{"status":"ok","timestamp":1702081051297,"user_tz":300,"elapsed":162,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Capsule Network Loss"],"metadata":{"id":"hO866snw4zXX"}},{"cell_type":"code","source":["class CapsNetLoss(nn.Module):\n","    def __init__(self, lmbda=0.5, m_positive=0.9, m_negative=0.1, recon_loss_scale_factor=0.0005, *args, **kwargs):\n","        super(CapsNetLoss, self).__init__(*args, **kwargs)\n","\n","        self.lmbda = lmbda\n","        self.m_positive = m_positive\n","        self.m_negative = m_negative\n","        self.recon_loss_scale_factor = recon_loss_scale_factor\n","        self.mse_loss = nn.MSELoss()\n","\n","    def forward(self, v, labels, images, reconstructed_images):\n","        return self.margin_loss(v, labels) + self.reconstruction_loss(images, reconstructed_images)\n","\n","    def margin_loss(self, v, labels):\n","        vk = torch.sqrt((v**2).sum(dim=-2, keepdim=True))\n","        # print(\"vk shape:\", vk.shape)\n","\n","        batch_size = v.size(0)\n","\n","        present_error = F.relu(self.m_positive - vk).view(batch_size, -1)**2\n","        absent_error = F.relu(vk - self.m_negative).view(batch_size, -1)**2\n","\n","        # print(\"present_error shape:\", present_error.shape)\n","        # print(\"absent_error shape:\", absent_error.shape)\n","\n","        Tk = F.one_hot(labels)\n","        # print(\"Tk shape:\", Tk.shape)\n","\n","        loss = (Tk * present_error) + (self.lmbda * (1 - Tk) * absent_error)\n","        loss = loss.sum(dim=1).mean()\n","\n","        return loss\n","\n","    def reconstruction_loss(self, images, reconstructed_images):\n","        return self.mse_loss(reconstructed_images, images) * self.recon_loss_scale_factor\n"],"metadata":{"id":"tpfoLi4aLKwZ","executionInfo":{"status":"ok","timestamp":1702081051297,"user_tz":300,"elapsed":3,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class BaseCapsuleNetwork(nn.Module):\n","    def __init__(self, in_img_c, in_img_h, in_img_w, n_class, *args, **kwargs):\n","        super(BaseCapsuleNetwork, self).__init__(*args, **kwargs)\n","\n","        self.conv_layer = ConvLayer(in_channels=in_img_c, kernel_size=9, stride=1)\n","\n","        conv_layer_out_dim1, conv_layer_out_dim2 = get_conv_out_dim(in_img_h, in_img_w, kernel_size=9, stride = 1)\n","\n","        self.primary_caps = PrimaryCapsLayer(out_channels=32, kernel_size=9, stride=2)\n","\n","        prim_caps_out_dim1, prim_caps_out_dim2 = get_conv_out_dim(conv_layer_out_dim1, conv_layer_out_dim2, kernel_size=9, stride = 2)\n","        n_prim_caps = prim_caps_out_dim1 * prim_caps_out_dim2 * 32\n","\n","        self.class_caps = ClassCapsLayer(n_class=n_class, n_prim_caps=n_prim_caps)\n","        self.routing_aggreement = RoutingByAggreement(n_in_caps=n_prim_caps, n_out_caps=n_class)\n","        self.caps_decoder = CapsDecoder(n_class, in_img_c, in_img_h, in_img_w)\n","\n","    def forward(self, images):\n","        conv_output = self.conv_layer(images)\n","        primary_caps_output = self.primary_caps(conv_output)\n","        class_caps_output = self.class_caps(primary_caps_output)\n","        pred = self.routing_aggreement(class_caps_output)\n","        reconstructed_img = self.caps_decoder(pred)\n","        return reconstructed_img, pred\n"],"metadata":{"id":"_pCjc5qTckX1","executionInfo":{"status":"ok","timestamp":1702081051297,"user_tz":300,"elapsed":2,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["base_capsnet_cifar10 = BaseCapsuleNetwork(3, 32, 32, n_class=10)\n","base_capsnet_cifar10 = base_capsnet_cifar10.to(device)\n","optimizer = Adam(base_capsnet_cifar10.parameters())\n","\n","train_loader, test_loader = image_dataloader(dataset='cifar10')\n","\n","criterion = CapsNetLoss()\n","n_epochs = 30"],"metadata":{"id":"HAzi15-uLJjy","executionInfo":{"status":"ok","timestamp":1702138429519,"user_tz":300,"elapsed":15213,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"933aac7c-b96a-4cf8-ddc6-ff5268616767"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 93353411.74it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# base_capsnet_mnist = BaseCapsuleNetwork(1, 28, 28, n_class=10)\n","# base_capsnet_mnist = base_capsnet_mnist.to(device)\n","# optimizer = Adam(base_capsnet_mnist.parameters())\n","\n","# train_loader, test_loader = image_dataloader(dataset='mnist')"],"metadata":{"id":"poSO8KNIEAyJ","executionInfo":{"status":"ok","timestamp":1702081069787,"user_tz":300,"elapsed":16,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["losses = []\n","\n","for epoch in range(1, n_epochs+1):\n","    base_capsnet_cifar10.train()\n","    train_loss = 0.0\n","\n","    for batch_idx, (images, labels) in enumerate(train_loader):\n","        batch_size = len(images)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        reconstructed_images, preds = base_capsnet_cifar10(images)\n","\n","        loss = criterion(preds, labels, images, reconstructed_images)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss\n","\n","        losses.append(loss)\n","\n","    print(\"Epoch {} - Average Training Loss: {}\".format(epoch, train_loss / len(train_loader)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":658},"id":"tSVXDJuaOfQB","executionInfo":{"status":"error","timestamp":1702141158550,"user_tz":300,"elapsed":2729041,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"outputId":"40b07578-3bb2-4cf3-d2c0-271852bcf0ee"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 - Average Training Loss: 0.3779374361038208\n","Epoch 2 - Average Training Loss: 0.3014434576034546\n","Epoch 3 - Average Training Loss: 0.26901787519454956\n","Epoch 4 - Average Training Loss: 0.24630065262317657\n","Epoch 5 - Average Training Loss: 0.227930948138237\n","Epoch 6 - Average Training Loss: 0.21207235753536224\n","Epoch 7 - Average Training Loss: 0.19771838188171387\n","Epoch 8 - Average Training Loss: 0.18229927122592926\n","Epoch 9 - Average Training Loss: 0.16912971436977386\n","Epoch 10 - Average Training Loss: 0.1543203592300415\n","Epoch 11 - Average Training Loss: 0.13978959619998932\n","Epoch 12 - Average Training Loss: 0.12578661739826202\n","Epoch 13 - Average Training Loss: 0.11042008548974991\n","Epoch 14 - Average Training Loss: 0.0959852859377861\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-70b3853ea926>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["base_capsnet_cifar10_flops, base_capsnet_cifar10_params = profile(base_capsnet_cifar10, inputs=(images,))\n","\n","print(\"Base CapsNet CIFAR10 Flops = {:.2f}\".format(base_capsnet_cifar10_flops))\n","print(\"Base CapsNet CIFAR10 Params = {:.2f}\".format( base_capsnet_cifar10_params))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"Z-rQ07qISJUD","executionInfo":{"status":"error","timestamp":1702138041687,"user_tz":300,"elapsed":162,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"outputId":"6840d0c2-7433-4875-df5e-b861f70337b6"},"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-3be1f148a452>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_capsnet_cifar10_flops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_capsnet_cifar10_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_capsnet_cifar10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Base CapsNet CIFAR10 Flops = {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_capsnet_cifar10_flops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Base CapsNet CIFAR10 Params = {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbase_capsnet_cifar10_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"]}]},{"cell_type":"code","source":["base_capsnet_cifar10.eval()\n","test_loss = 0\n","\n","for batch_id, (data, target) in enumerate(test_loader):\n","    batch_size = len(images)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    reconstructed_images, preds, masked_preds = base_capsnet_cifar10(images)\n","    loss = criterion(preds, labels, images, reconstructed_images)\n","\n","    test_loss += loss\n","\n","    if batch_id % 100 == 0:\n","        print(\"Test Accuracy:\", sum(np.argmax(masked_preds.data.cpu().numpy(), 1) ==\n","                                np.argmax(labels.data.cpu().numpy())) / float(batch_size))\n","\n","print(\"Test Loss: \", test_loss / len(test_loader))"],"metadata":{"id":"_L7c0P6OaiJu","executionInfo":{"status":"error","timestamp":1702082330358,"user_tz":300,"elapsed":344,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"colab":{"base_uri":"https://localhost:8080/","height":253},"outputId":"ad4d84c1-314a-46a3-c539-e34d089acf77"},"execution_count":20,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a37064fef706>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mreconstructed_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_capsnet_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}]},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","\n","def plot_images_separately(images):\n","    fig = plt.figure()\n","    for j in range(1, 7):\n","        ax = fig.add_subplot(1, 6, j)\n","        image = images[j-1]\n","        ax.imshow(image.T)\n","        plt.xticks(np.array([]))\n","        plt.yticks(np.array([]))\n","    plt.show()"],"metadata":{"id":"C-b5HngHfDLw","executionInfo":{"status":"ok","timestamp":1702141166157,"user_tz":300,"elapsed":136,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["plot_images_separately(images[:6, :].cpu())"],"metadata":{"id":"qtqTaY8LfGp4","executionInfo":{"status":"error","timestamp":1702141169293,"user_tz":300,"elapsed":1085,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"colab":{"base_uri":"https://localhost:8080/","height":500},"outputId":"1779f00e-1a8e-4231-b19e-a3a2dc8e593d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-15cb3a95207b>:9: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n","  ax.imshow(image.T)\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-06927827d69a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_images_separately\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-15cb3a95207b>\u001b[0m in \u001b[0;36mplot_images_separately\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["recon_image = reconstructed_images[4]\n","recon_image = recon_image.cpu().detach().numpy()\n","\n","plt.imshow(recon_image.T)"],"metadata":{"id":"_SqwYQjuibrg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa9ed90e-2b04-4df9-9c80-b622a8bf46b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x78894f7c7370>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","print(' '.join(f'{classes[labels[j]]:5s}' for j in range(100)))"],"metadata":{"id":"IUamYcUhikMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z5Y6sSMWjngD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Sangita Try\n"],"metadata":{"id":"OWEj1bnFjYWO"}},{"cell_type":"code","source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.optim import Adam\n","from torchvision import datasets, transforms\n","\n","USE_CUDA = True"],"metadata":{"id":"W7e7_aUSjbZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Mnist:\n","    def __init__(self, batch_size):\n","        dataset_transform = transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])\n","\n","        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=dataset_transform)\n","        test_dataset = datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)\n","\n","        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"],"metadata":{"id":"CMvx-xkAjgMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ConvLayer(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n","        super(ConvLayer, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=kernel_size,\n","                               stride=1\n","                             )\n","\n","    def forward(self, x):\n","        return F.relu(self.conv(x))"],"metadata":{"id":"mIx42NN3jix3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PrimaryCaps(nn.Module):\n","    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n","        super(PrimaryCaps, self).__init__()\n","\n","        self.capsules = nn.ModuleList([\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0)\n","                          for _ in range(num_capsules)])\n","\n","    def forward(self, x):\n","        u = [capsule(x) for capsule in self.capsules]\n","        u = torch.stack(u, dim=1)\n","        u = u.view(x.size(0), 32 * 6 * 6, -1)\n","        return self.squash(u)\n","\n","    def squash(self, input_tensor):\n","        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n","        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n","        return output_tensor"],"metadata":{"id":"pwfNwZlFjmzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DigitCaps(nn.Module):\n","    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n","        super(DigitCaps, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.num_routes = num_routes\n","        self.num_capsules = num_capsules\n","\n","        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n","\n","        W = torch.cat([self.W] * batch_size, dim=0)\n","        u_hat = torch.matmul(W, x)\n","\n","        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n","        if USE_CUDA:\n","            b_ij = b_ij.cuda()\n","\n","        num_iterations = 3\n","        for iteration in range(num_iterations):\n","            c_ij = F.softmax(b_ij)\n","            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n","\n","            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n","            v_j = self.squash(s_j)\n","\n","            if iteration < num_iterations - 1:\n","                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n","                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n","\n","        return v_j.squeeze(1)\n","\n","    def squash(self, input_tensor):\n","        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n","        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n","        return output_tensor"],"metadata":{"id":"k4am3BzTjq9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","\n","        self.reconstraction_layers = nn.Sequential(\n","            nn.Linear(16 * 10, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 784),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x, data):\n","        classes = torch.sqrt((x ** 2).sum(2))\n","        classes = F.softmax(classes)\n","\n","        _, max_length_indices = classes.max(dim=1)\n","        masked = Variable(torch.sparse.torch.eye(10))\n","        if USE_CUDA:\n","            masked = masked.cuda()\n","        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n","\n","        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n","        reconstructions = reconstructions.view(-1, 1, 28, 28)\n","\n","        return reconstructions, masked"],"metadata":{"id":"eS8CkBL8jwCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CapsNet(nn.Module):\n","    def __init__(self):\n","        super(CapsNet, self).__init__()\n","        self.conv_layer = ConvLayer()\n","        self.primary_capsules = PrimaryCaps()\n","        self.digit_capsules = DigitCaps()\n","        self.decoder = Decoder()\n","\n","        self.mse_loss = nn.MSELoss()\n","\n","    def forward(self, data):\n","        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n","        reconstructions, masked = self.decoder(output, data)\n","        return output, reconstructions, masked\n","\n","    def loss(self, data, x, target, reconstructions):\n","        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n","\n","    def margin_loss(self, x, labels, size_average=True):\n","        batch_size = x.size(0)\n","\n","        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n","\n","        left = F.relu(0.9 - v_c).view(batch_size, -1)\n","        right = F.relu(v_c - 0.1).view(batch_size, -1)\n","\n","        loss = labels * left + 0.5 * (1.0 - labels) * right\n","        loss = loss.sum(dim=1).mean()\n","\n","        return loss\n","\n","    def reconstruction_loss(self, data, reconstructions):\n","        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n","        return loss * 0.0005"],"metadata":{"id":"ata1eu8jj0YT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["capsule_net = CapsNet()\n","if USE_CUDA:\n","    capsule_net = capsule_net.cuda()\n","optimizer = Adam(capsule_net.parameters())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"avd4_cfxj2CE","executionInfo":{"status":"error","timestamp":1701890314923,"user_tz":300,"elapsed":14,"user":{"displayName":"Siddharth Sahu","userId":"13238428815416750697"}},"outputId":"6978ee4a-c1df-4dab-f7de-f05ad47ed0c6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-48a6d449d17b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcapsule_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcapsule_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":["batch_size = 100\n","mnist = Mnist(batch_size)\n","\n","n_epochs = 30\n","\n","\n","for epoch in range(n_epochs):\n","    capsule_net.train()\n","    train_loss = 0\n","    for batch_id, (data, target) in enumerate(mnist.train_loader):\n","\n","        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n","        data, target = Variable(data), Variable(target)\n","\n","        if USE_CUDA:\n","            data, target = data.cuda(), target.cuda()\n","\n","        optimizer.zero_grad()\n","        output, reconstructions, masked = capsule_net(data)\n","        loss = capsule_net.loss(data, output, target, reconstructions)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.data[0]\n","\n","        if batch_id % 100 == 0:\n","            print \"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n","                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n","\n","    print train_loss / len(mnist.train_loader)\n","\n","    capsule_net.eval()\n","    test_loss = 0\n","    for batch_id, (data, target) in enumerate(mnist.test_loader):\n","\n","        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n","        data, target = Variable(data), Variable(target)\n","\n","        if USE_CUDA:\n","            data, target = data.cuda(), target.cuda()\n","\n","        output, reconstructions, masked = capsule_net(data)\n","        loss = capsule_net.loss(data, output, target, reconstructions)\n","\n","        test_loss += loss.data[0]\n","\n","        if batch_id % 100 == 0:\n","            print \"test accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n","                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n","\n","    print test_loss / len(mnist.test_loader)\n"],"metadata":{"id":"F0iG7nUskE-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","\n","def plot_images_separately(images):\n","    \"Plot the six MNIST images separately.\"\n","    fig = plt.figure()\n","    for j in xrange(1, 7):\n","        ax = fig.add_subplot(1, 6, j)\n","        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n","        plt.xticks(np.array([]))\n","        plt.yticks(np.array([]))\n","    plt.show()"],"metadata":{"id":"qG0NLCKNkMnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_images_separately(data[:6,0].data.cpu().numpy())"],"metadata":{"id":"ISyPI8AWkPR2"},"execution_count":null,"outputs":[]}]}